\documentclass{acm_proc_article-sp}

\begin{document}

\title{Minimizing Posting List Traversal\titlenote{A summary of previous papers and a suggestion of new ideas}}

\numberofauthors{1} 
\author{
\alignauthor
Edward Hills\\\affaddr{University of Otago}\\
       \affaddr{Dunedin, New Zealand}\\
       \email{ehills@cs.otago.ac.nz}}
\date{18 May 2012}

\maketitle
\begin{abstract}

Information Retrieval is primarily concerned with searching through a document collection given a query, and returning a set of documents which could be relevant to the users request. 

This task usually requires searching through the entire document collection to find which documents may be relevant. In large document collections this can be time consuming and costly. This paper looks at ways to minimize the number of documents that are examined but still being effective in returning the documents most likely to be relevant.

\end{abstract}

\section{Introduction}

Searching through document collections which are large can mean when it comes to query time that all documents that are indexed by one or more of the search terms will be examined. When you have a large document collection such as the TREC terabyte collection, searching every document that is indexed by even a relatively rare term can mean searching through thousands or even millions of documents.

By reducing the amount of documents searched by a particular query you do run the risk of limiting the amount of documents returned which are relevant to the user. Keeping the accuracy of your list returned is a top priority when searching through a minimal amount of documents. 

There are a number of techniques in which the document list traversal can be minimized while still maintaining high accuracy. This paper looks at three techniques: 
\begin{enumerate}
\item Index ordering by query-independent measures, Ferguson and Smeaton
\item The Nearest Neighbour Problem In Information Retrieval, van Rijsbergenand Smeaton
\item Filtered Document Retrieval with Frequency-Sorted Indexes, Persin, Zobel and Sacks-Davis
\end{enumerate}

I will examine document minimization further and discuss the main research questions and contributions of each paper in 2., talk about how these techniques are linked in 3., then propose two new ideas in 4. and discuss my final findings in the Conclusion.

\section{Document Minimization}

By minimizing the document collection that must be evaluated we can avoid unnecessary calculation which slows down performance, lengthens postings list and increases the inverted index file size.

This area of research has been looked into since the early '80s and a multitude of techniques have arisen ranging from Query caching (Lempel and Moran (2003)) and two-tier architectures (Fagni, Perego, Silvestri, and Orlando (2006)) to nearest neighbour searching (Smeaton \& van Rijsbergen (1981)) which we talk about lately.

These different techniques are not entirely mutually exclusive as some may think but in some cases can be built upon layer by layer such as the \emph{quit} and \emph{continue} strategies (Moffat \& Zobel, 1996) which are used in a variety of different techniques. 

With todays web based document collection growing larger and larger the need to weed out documents which are spam or of poor quality to the user needs to be addressed. By removing documents which are of poor quality not only increases the performance of the search engine (as there are less documents to search through given a certain query) but also increases the likelihood that only relevant documents are returned to the user. Removing web documents from a document collection is discussed in the next section.

\subsection{Research Questions \& Contributions}
\subsubsection{Index ordering by query-independent measures}
This paper from Ferguson et al. aims to remove html documents from its index in a query-independent manner. This means that instead of at query-time and having to wade through all documents for each query and decide at query-time that you do not want that document, you instead check to see if you want that document in the first place. 

Ferguson et al. have come up with a range of heuristics for determining which documents are likely to be spam or of poor quality. A common formula for finding good quality documents is PageRank which gives a high priority to documents it thinks is best. PageRank is used as a query-independent document filter but this paper hopes to do better in the quality of documents returned.

Some heuristics developed include:
\begin{itemize}
\item Access counts
\item Information-to-noise ratio
\item Document cohesiveness
\item Document structure and layout
\item Term-specific sorting
\item Global BM25 sorting
\end{itemize}
\emph{See Ferguson et al. for details about all heuristics.}

The two main new contributions of this paper are \emph{term-specific sorting} and \emph{global BM25 sorting}.

Term-specific sorting uses the term weighting approach of BM25 and applies that to each document in each postings list separately and provides a pre-calculated BM25 score for each posting list. This is relatively close to what would be used at query-time however is lacking the summation of each query term score. It is also important to note that there is no need to maintain information such as \emph{df(t)}.

Global BM25 sorting is similar to that above but keeps track of the importance of each term. This does cause problems however as it doesn't optimize for Zipf's Law  (Zipf, 1932) as it gives a higher rating for terms which are very infrequent rather than moderately infrequent. To get around this they imposed a threshold in which terms which do not appear in a document more times than the threshold will not be added. To calculate the threshold they used a large query log from the \emph{Excite} search engine from 1999. They generated two alternative global BM25 measures:
 
\emph{Global BM25 Query Log Terms (QLTs):} global BM25 scores by including only terms used in the query log.

\emph{Global BM25 Query Log Term Frequency (QLTF):} considers the frequency that each term occurs within the query log (i.e. idf).

The results of this paper found that by combining the best static measure (which turned out to be access counts) and the global BM25 sorting technique at retrieval-time (as well as normal BM25 at query-time) that they can achieve a higher P10 value than a conventional IR index by processing only 15\% of the postings list. This is a huge reduction of 85\%, however the \emph{Mean-average precision (MAP)} was lower than that of the conventional index (0.282 compared with 0.304). However the P10 and MAP trade-off is worth it as most uses are not likely to go past the first page of results so the lower MAP value is unlikely to have a real effect on the user.

\subsubsection{The Nearest Neighbour Problem in Information Retrieval}

This paper from Smeaton et al. aims to reduce the number of documents that need to be scanned when scanning through the postings list for each term in the given query. It does this by using nearest neighbour classification and proposes a new algorithm for doing this.

The hopes of this paper was to develop an algorithm which would solve the nearest neighbour problem in IR. That is given a document it wishes to find all nearest neighbours of that up to a given threshold. This algorithm also has the added benefit of only checking the same document once, even if indexed multiple times by multiple terms in the search query.

I describe the algorithm below:

Maintain two sets, \emph{R} and \emph{S}. \emph{R} contains all documents which are final candidates for the set of nearest neighbours. Where the maximum size of \emph{R} is given by the user (lets assume one for now). \emph{S} contains the set of all documents discovered so far whether in \emph{R} or not.

Then given a query containing a set of search terms, order the terms based on the order of their increasing frequencies of occurrence within the entire document collection. Then add every document containing the term into \emph{S}, the nearest neighbour will be found among this set.

By assuming that the number of co-occurrences of index terms between a document and a query is zero, then the similarity is zero, then it is possible to remove documents which are not indexed by at least one search term from the nearest neighbour search.

When checking each document found for a particular term, first check it is in \emph{S} and if it is not then calculate a similarity using one of the formula below and add it to \emph{S}.

Similarity Measures:
\begin{itemize}
\item Simple
\item Ivie
\item Dice
\item Cosine
\item Jaccard
\item Overlap
\end{itemize}

If the value returned is greater than the best value so far then this document becomes a candidate and is added to \emph{R}. After looking through each document for a given term it is then possible to calculate the maximum possible similarity value between among the documents which haven't been discovered yet. If this upper-bound that is calculated is less than the best value found so far then terminate.

This surprisingly fairly simple algorithm managed to save the number of document comparisons needed by about 50-60\%. It is important the document collection does affect the results as explained in the paper, however an average amount of comparisons needed of about 40\% was accepted.

\subsubsection{Filtered Document Retrieval with Frequency-Sorted Indexes}

This paper by Persin et al. aimed to determine which documents are more likely to be highly ranked by organizing the inverted file by descending term frequency. By doing this the test data used 2\% of memory that a standard implementation would, cpu and disk use was also reduced. This paper also showed that frequency sorting can decrease the index size regardless of compression used.



\subsection{Relationship}
Talk about how all three papers are related to each other

\section{Future Work}
I came up with two ideas in this field which are roughly to do with this... blah blah blah

\subsection{Question 1}
Well the first question i wanted answered is, can we do this... blah blah blah. the best way to do this would be combine this this and that and then it may be possible to imporve performance or not blah blah blah

\subsection{Question 2}
Well the first question i wanted answered is, can we do this... blah blah blah. the best way to do this would be combine this this and that and then it may be possible to imporve performance or not blah blah blah

\section{Conclusions}
You can see that this paper has described a multitude of different ways in which we can minimize the cost of transfering or merging the queries in a distributed IR environment. by taking points from paper 1 and paper 2 they can be combined and blah blah blah. Paper 3 talks aboue the architecture and we can see that this is important because of blah.

I came up with my own 2 points and found that blah blah blah

\bibliographystyle{abbrv}
\bibliography{asgn2bib} 

\subsection{References}
Generated by bibtex from your ~.bib file.  Run latex,
then bibtex, then latex twice (to resolve references)
to create the ~.bbl file.  Insert that ~.bbl file into
the .tex source file and comment out
the command \texttt{{\char'134}thebibliography}.
\balancecolumns
% That's all folks!
\end{document}
